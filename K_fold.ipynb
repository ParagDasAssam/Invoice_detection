{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of ROBOFLOW-tensorflow-object-detection-faster-rcnn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FIqnjbWYsuQw",
        "2FKFq8RXs6bs",
        "MFyCeiBb9BbS"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOqjQsvnTNRO",
        "colab_type": "text"
      },
      "source": [
        "## Setting the pre-requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VzNpNztQurn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "outputId": "a3312a69-f536-44e2-b1ad-09fae14154c0"
      },
      "source": [
        "!pip install tensorflow_gpu==1.15"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_gpu==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 43kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 53.0MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 53.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.32.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.18.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.8.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (3.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow_gpu==1.15) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (3.2.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (50.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=b18acf138244afa365cd11309c966d460d2a4b8e9195ef08d61ea696cf3a1573\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, tensorboard, tensorflow-estimator, gast, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZYSny--SUnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you forked the repo, you can replace the link.\n",
        "repo_url = 'https://github.com/roboflow-ai/tensorflow-object-detection-faster-rcnn'\n",
        "\n",
        "# Number of training steps - 1000 will train very quickly, but more steps will increase accuracy.\n",
        "num_steps = 5000  # 200000 to improve\n",
        "\n",
        "# Number of evaluation steps.\n",
        "#num_eval_steps = 50\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "        'faster_rcnn_inception_v2': {\n",
        "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
        "        'pipeline_file': 'demo.config',\n",
        "        'batch_size': 1\n",
        "    }\n",
        "}\n",
        "\n",
        "# Pick the model you want to use\n",
        "# Select a model in `MODELS_CONFIG`.\n",
        "selected_model = 'faster_rcnn_inception_v2'\n",
        "\n",
        "# Name of the object detection model to use.\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "# Name of the pipline file in tensorflow object detection API.\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n",
        "batch_size = MODELS_CONFIG[selected_model]['batch_size']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAfWCBq5SUpv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 995
        },
        "outputId": "0413ac82-4758-4521-faaa-1e2d2314a25f"
      },
      "source": [
        "import os\n",
        "\n",
        "%cd /content\n",
        "\n",
        "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd {repo_dir_path}\n",
        "!git pull\n",
        "\n",
        "!pip install tf_slim\n",
        "\n",
        "\n",
        "%cd /content\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "!pip install -q pycocotools\n",
        "\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "\n",
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'tensorflow-object-detection-faster-rcnn'...\n",
            "remote: Enumerating objects: 885, done.\u001b[K\n",
            "remote: Total 885 (delta 0), reused 0 (delta 0), pack-reused 885\u001b[K\n",
            "Receiving objects: 100% (885/885), 24.84 MiB | 8.68 MiB/s, done.\n",
            "Resolving deltas: 100% (412/412), done.\n",
            "/content/tensorflow-object-detection-faster-rcnn\n",
            "Already up to date.\n",
            "Collecting tf_slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf_slim) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n",
            "/content\n",
            "Selecting previously unselected package python-bs4.\n",
            "(Reading database ... 144600 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (5.1.0-1ubuntu0.3) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-bs4 (4.6.0-1) ...\n",
            "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Setting up python-olefile (0.45.1-1) ...\n",
            "Setting up python-pil:amd64 (5.1.0-1ubuntu0.3) ...\n",
            "Setting up python-webencodings (0.5-2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up python-html5lib (0.999999999-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5TV8lY-Ta9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "37a2b6db-5ed8-43af-b6d6-88baea85fbe2"
      },
      "source": [
        "%cd /content/models/research\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = '/content/models/research/pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)\n",
        "\n",
        "!echo {DEST_DIR}\n",
        "!ls -alh {DEST_DIR}\n",
        "\n",
        "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
        "fine_tune_checkpoint\n",
        "\n",
        "pipeline_fname = os.path.join('/content/models/research/object_detection/samples/configs/', pipeline_file)\n",
        "print(pipeline_fname)\n",
        "\n",
        "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n",
            "/content/models/research/pretrained_model\n",
            "total 111M\n",
            "drwxr-xr-x  3 345018 5000 4.0K Feb  1  2018 .\n",
            "drwxr-xr-x 24 root   root 4.0K Sep 17 04:31 ..\n",
            "-rw-r--r--  1 345018 5000   77 Feb  1  2018 checkpoint\n",
            "-rw-r--r--  1 345018 5000  55M Feb  1  2018 frozen_inference_graph.pb\n",
            "-rw-r--r--  1 345018 5000  51M Feb  1  2018 model.ckpt.data-00000-of-00001\n",
            "-rw-r--r--  1 345018 5000  16K Feb  1  2018 model.ckpt.index\n",
            "-rw-r--r--  1 345018 5000 5.5M Feb  1  2018 model.ckpt.meta\n",
            "-rw-r--r--  1 345018 5000 3.2K Feb  1  2018 pipeline.config\n",
            "drwxr-xr-x  3 345018 5000 4.0K Feb  1  2018 saved_model\n",
            "/content/models/research/object_detection/samples/configs/demo.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMcw9N2eQYVI",
        "colab_type": "text"
      },
      "source": [
        "## Create different sets of train and eval datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBId14aCQE5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c851aed8-15e0-4f4f-f540-ab50bcfcd76f"
      },
      "source": [
        "%cd /content/\n",
        "\n",
        "# change the name when re-running\n",
        "# name of the file where the data will go when we do splitting\n",
        "split_output_file_name = \"Splited_record\"\n",
        "!mkdir {split_output_file_name}\n",
        "\n",
        "# Change the output file name\n",
        "out_put_file_name = \"Records\"\n",
        "\n",
        "# Number of splits\n",
        "num_of_splits = 6\n",
        "\n",
        "# Main tfrecord file name\n",
        "file_path = \"/content/tensorflow-object-detection-faster-rcnn/Original_dataset/eval_default.tfrecord\"\n",
        "\n",
        "# # dir where the split datasets will be stored\n",
        "# output_dir_name = split_output_file_name"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DClRSr5bXKNk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "189bca0c-bed8-4df8-8eec-bd35856c0d61"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E60POlDWQhA5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "4e5f52b3-3d91-4f42-b0c2-2bdc9175be3f"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# sum of records\n",
        "def give_sum_of_records(file_path):\n",
        "  print(\"File name: \",file_path.split(\"/\")[-1],\", Number of records: \",sum(1 for _ in tf.data.TFRecordDataset(file_path)))\n",
        "\n",
        "# Split the record file\n",
        "def split_the_record(file_path,num_of_splits,output_dir_name):\n",
        "  %cd {output_dir_name}\n",
        "  raw_dataset = tf.data.TFRecordDataset(file_path)\n",
        "  shards = num_of_splits\n",
        "  for i in range(shards):\n",
        "      writer = tf.data.experimental.TFRecordWriter(f\"file_{i}.tfrecord\")\n",
        "      print(writer)\n",
        "      writer.write(raw_dataset.shard(shards, i))\n",
        "\n",
        "split_the_record(file_path,num_of_splits,split_output_file_name)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Splited_record\n",
            "<tensorflow.python.data.experimental.ops.writers.TFRecordWriter object at 0x7fceeea707b8>\n",
            "<tensorflow.python.data.experimental.ops.writers.TFRecordWriter object at 0x7fceeeac6f98>\n",
            "<tensorflow.python.data.experimental.ops.writers.TFRecordWriter object at 0x7fceeea707b8>\n",
            "<tensorflow.python.data.experimental.ops.writers.TFRecordWriter object at 0x7fceeeac6f98>\n",
            "<tensorflow.python.data.experimental.ops.writers.TFRecordWriter object at 0x7fceeea707b8>\n",
            "<tensorflow.python.data.experimental.ops.writers.TFRecordWriter object at 0x7fceeeac6f98>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OigfEEgwQhEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# listing the files created\n",
        "\n",
        "file_path = \"/content/\"+output_dir_name+\"/\"\n",
        "list_of_records = os.listdir(file_path)\n",
        "list_of_records.sort()\n",
        "list_of_records = [file_path+x for x in list_of_records]\n",
        "\n",
        "for i in range(len(list_of_records)):\n",
        "  print(list_of_records[i])\n",
        "\n",
        "print(\"\\nNumber of records:\",len(list_of_records))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu14sZhFQloy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!mkdir /content/{out_put_file_name}\n",
        "\n",
        "%cd /content/{out_put_file_name}\n",
        "\n",
        "for i in range(0,len(list_of_records)):\n",
        "  print(\"Eval:\",list_of_records[i])\n",
        "  temp = [list_of_records[x] for x in range(len(list_of_records)) if x != i]\n",
        "  print(\"train:\",temp,\"\\n\")\n",
        "\n",
        "  # For eval dataset\n",
        "  list_of_tfrecord_files = list_of_records[i]\n",
        "  dataset = tf.data.TFRecordDataset(list_of_tfrecord_files)\n",
        "\n",
        "  # # Save dataset to .tfrecord file\n",
        "  filename = 'eval_'+str(i)+'.tfrecord'\n",
        "\n",
        "  writer = tf.data.experimental.TFRecordWriter(filename)\n",
        "  writer.write(dataset)\n",
        "\n",
        "  # # For training dataset\n",
        "  list_of_tfrecord_files = temp\n",
        "  dataset = tf.data.TFRecordDataset(list_of_tfrecord_files)\n",
        "\n",
        "  # # Save dataset to .tfrecord file\n",
        "  filename = 'train_'+str(i)+'.tfrecord'\n",
        "\n",
        "  writer = tf.data.experimental.TFRecordWriter(filename)\n",
        "  writer.write(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKQVvTUJQlrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = \"/content/\"+out_put_file_name+\"/\"\n",
        "\n",
        "list_of_records = os.listdir(file_path)\n",
        "list_of_records.sort()\n",
        "list_of_records = [file_path+x for x in list_of_records]\n",
        "\n",
        "for record in list_of_records:\n",
        "  give_sum_of_records(record)\n",
        "\n",
        "print(\"\\nNumber of records:\",len(list_of_records))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq6KprKkTGyh",
        "colab_type": "text"
      },
      "source": [
        "## Now for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFQpVhaCSUuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOTE: Update these TFRecord names from \"cells\" and \"cells_label_map\" to your files!\n",
        "train_record_fname = '/content/tensorflow-object-detection-faster-rcnn/data/data.record'\n",
        "label_map_pbtxt_fname = '/content/tensorflow-object-detection-faster-rcnn/data/demo.pbtxt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlIeSeo5S6JB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())\n",
        "\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw3uV1j9SUxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open(pipeline_fname, 'w') as f:\n",
        "    \n",
        "    # fine_tune_checkpoint\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test.\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    \n",
        "    # label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "    f.write(s)\n",
        "\n",
        "!cat {pipeline_fname}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhUPeVMSQlxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}